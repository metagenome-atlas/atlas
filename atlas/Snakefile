import os
import re
import sys
import tempfile
import pandas as pd
import numpy as np

from snakemake.utils import logger, min_version

sys.path.append(
    os.path.join(os.path.dirname(os.path.abspath(workflow.snakefile)), "scripts")
)
import utils

from conf import update_config

config = update_config(config)
from default_values import *  # LOAD HARDCODED values and EGGNOG_HEADER
from conf import load_sample_table

sampleTable = load_sample_table()

# minimum required snakemake version
min_version("6.1")


container: "docker://continuumio/miniconda3:4.4.10"


def get_temp_dir(config):
    if config.get("tmpdir"):
        tmp_dir = config["tmpdir"]
    else:
        tmp_dir = tempfile.gettempdir()
    return tmp_dir


def io_params_for_tadpole(io, key="in"):
    """This function generates the input flag needed for bbwrap/tadpole for all cases
    possible for get_quality_controlled_reads.

    params:
        io  input or output element from snakemake
        key 'in' or 'out'

        if io contains attributes:
            se -> in={se}
            R1,R2,se -> in1={R1},se in2={R2}
            R1,R2 -> in1={R1} in2={R2}

    """
    N = len(io)
    if N == 1:
        flag = f"{key}1={io[0]}"
    elif N == 2:
        flag = f"{key}1={io[0]} {key}2={io[1]}"
    elif N == 3:
        flag = f"{key}1={io[0]},{io[2]} {key}2={io[1]}"
    else:
        logger.critical(
            (
                "File input/output expectation is one of: "
                "1 file = single-end/ interleaved paired-end "
                "2 files = R1,R2, or"
                "3 files = R1,R2,se"
                "got: {n} files:\n{}"
            ).format("\n".join(io), n=len(io))
        )
        sys.exit(1)
    return flag


def input_paired_only(files):
    """
    return only paired interleaved or not
    """

    if not PAIRED_END:
        raise IOError("Ask for paired end samples if not paired end workflow")

    return files[: min(len(files), 2)]


def input_params_for_bbwrap(input):

    if len(input) == 3:
        return f"in1={input[0]},{input[2]} in2={input[1]},null"
    else:
        return io_params_for_tadpole(input)


# if config.get("workflow") != "download":

#    config = update_config_file_paths(config)
TMPDIR = get_temp_dir(config)
SAMPLES = sampleTable.index.values
SKIP_QC = False
# GROUPS = sampleTable.BinGroup.unique()
def get_alls_samples_of_group(wildcards):
    group_of_sample = sampleTable.loc[wildcards.sample, "BinGroup"]

    return list(sampleTable.loc[sampleTable.BinGroup == group_of_sample].index)


PAIRED_END = sampleTable.columns.str.contains("R2").any() or config.get(
    "interleaved_fastqs", False
)


colum_headers_QC = sampleTable.columns[sampleTable.columns.str.startswith("Reads_QC_")]
if len(colum_headers_QC) >= 1:
    MULTIFILE_FRACTIONS = list(colum_headers_QC.str.replace("Reads_QC_", ""))

    if (len(MULTIFILE_FRACTIONS) == 1) and config.get("interleaved_fastqs", False):
        MULTIFILE_FRACTIONS = ["R1", "R2"]

else:
    MULTIFILE_FRACTIONS = ["R1", "R2", "se"] if PAIRED_END else ["se"]

colum_headers_raw = sampleTable.columns[
    sampleTable.columns.str.startswith("Reads_raw_")
]
if len(colum_headers_raw) == 0:
    SKIP_QC = True

    logger.info("Didn't find raw reads in sampleTable - skip QC")
    RAW_INPUT_FRACTIONS = MULTIFILE_FRACTIONS
else:
    RAW_INPUT_FRACTIONS = ["R1", "R2"] if PAIRED_END else ["se"]


if (len(colum_headers_raw) == 0) and (len(colum_headers_QC) == 0):

    raise IOError(
        "Either raw reas or QC reads need to be in the sample table. "
        "I din't find any collums with 'Reads_raw_<fraction>' or 'Reads_QC_<fraction>'  "
    )


class FileNotInSampleTableException(Exception):
    """
    Exception with sampleTable
    """

    def __init__(self, message):
        super(FileNotInSampleTableException, self).__init__(message)


def get_files_from_sampleTable(sample, Headers):
    """
    Function that gets some filenames form the sampleTable for a given sample and Headers.
    It checks various possibilities for errors and throws either a
    FileNotInSampleTableException or a IOError, when something went really wrong.
    """

    if not (sample in sampleTable.index):
        raise IOError(f"Sample name {sample} is not in sampleTable")

    Error_details = f"\nsample: {sample}\nFiles: {Headers}"

    if type(Headers) == str:
        Headers = [Headers]

    NheadersFound = sampleTable.columns.isin(Headers).sum()

    if NheadersFound == 0:
        raise FileNotInSampleTableException(
            f"None of the Files ar in sampleTable, they should be added to the sampleTable later in the workflow"
            + Error_details
        )
    elif NheadersFound < len(Headers):
        raise IOError(
            f"Not all of the Headers are in sampleTable, found only {NheadersFound}, something went wrong."
            + Error_details
        )

    files = sampleTable.loc[sample, Headers]

    if files.isnull().all():
        raise FileNotInSampleTableException(
            "The following files were not available for this sample in the SampleTable"
            + Error_details
        )

    elif files.isnull().any():
        raise IOError(
            f"Not all of the files are in sampleTable, something went wrong."
            + Error_details
        )

    return list(files)


def get_quality_controlled_reads(wildcards):
    """Gets quality controlled reads for two cases. When preprocessed with
    ATLAS, returns R1, R2 and se fastq files or just se. When preprocessed
    externaly and run ATLAS workflow assembly, we expect R1, R2 or se.
    """

    if config.get("interleaved_fastqs", False) and SKIP_QC:
        QC_Headers = ["Reads_QC_se"]
    else:
        QC_Headers = ["Reads_QC_" + f for f in MULTIFILE_FRACTIONS]

    try:
        return get_files_from_sampleTable(wildcards.sample, QC_Headers)
    except FileNotInSampleTableException:

        # return files as named by atlas pipeline
        return expand(
            "{sample}/sequence_quality_control/{sample}_QC_{fraction}.fastq.gz",
            fraction=MULTIFILE_FRACTIONS,
            sample=wildcards.sample,
        )


include: "rules/download.smk"  # contains hard coded variables
include: "rules/qc.smk"
include: "rules/assemble.smk"
include: "rules/binning.smk"
include: "rules/genomes.smk"
include: "rules/genecatalog.smk"
include: "rules/cat_taxonomy.smk"
include: "rules/gtdbtk.smk"


CONDAENV = "envs"  # overwrite definition in download.smk


localrules:
    all,
    qc,
    assembly_one_sample,
    assembly,
    genomes,


rule all:
    input:
        "finished_QC",
        "finished_assembly",
        "finished_binning",
        "finished_genomes",
        "finished_genecatalog",
        "genomes/annotations/gene2genome.tsv.gz",


rule genecatalog:
    input:
        "Genecatalog/gene_catalog.fna",
        "Genecatalog/gene_catalog.faa",
        "Genecatalog/clustering/orf2gene.tsv.gz",
        #"Genecatalog/counts/median_coverage.tsv.gz",
        # expand("Genecatalog/annotation/single_copy_genes_{domain}.tsv",domain=['bacteria','archaea']),
        "Genecatalog/annotations/eggNog.tsv.gz",
    output:
        temp(touch("finished_genecatalog")),


def get_genome_annotations():

    annotation_file_names = {
        "cat_taxonomy": "genomes/taxonomy/taxonomy.tsv",
        "ssu": "genomes/SSU/ssu_summary.tsv",
        "checkm_taxonomy": "genomes/checkm/taxonomy.tsv",
        "checkm_tree": "genomes/tree/checkm.nwk",
        "gtdb_tree": "genomes/tree/finished_gtdb_trees",
        "gtdb_taxonomy": "genomes/taxonomy/gtdb/classify",
        "genes": "genomes/annotations/genes/predicted",
    }

    annotations_requested = config["annotations"]

    try:
        annotations_files = ["genomes/checkm/completeness.tsv"] + [
            annotation_file_names[an] for an in annotations_requested
        ]

    except Exception as e:

        raise IOError(
            "Error in annotations requested, check config file 'annotations' "
        ) from e

    return annotations_files


rule genomes:
    input:
        "genomes/Dereplication/dereplicated_genomes",
        "genomes/counts/median_coverage_genomes.tsv",
        "genomes/counts/raw_counts_genomes.tsv",
        "genomes/clustering/allbins2genome.tsv",
        * get_genome_annotations(),
        "finished_binning",
    output:
        temp(touch("finished_genomes")),


rule binning:
    input:
        expand(
            "{sample}/binning/{binner}/cluster_attribution.tsv",
            binner=config["final_binner"],
            sample=SAMPLES,
        ),
        expand("reports/bin_report_{binner}.html", binner=config["final_binner"]),
        "genomes/checkm/checkm_all_bins.tsv",
        "finished_assembly",
    output:
        temp(touch("finished_binning")),


rule assembly_one_sample:
    input:
        "{sample}/{sample}_contigs.fasta",
        "{sample}/sequence_alignment/{sample}.bam",
        "{sample}/assembly/contig_stats/postfilter_coverage_stats.txt",
        "{sample}/assembly/contig_stats/prefilter_contig_stats.txt",
        "{sample}/assembly/contig_stats/final_contig_stats.txt",
    output:
        touch("{sample}/finished_assembly"),


rule assembly:
    input:
        expand("{sample}/finished_assembly", sample=SAMPLES),
        "reports/assembly_report.html",
        "finished_QC",
    output:
        temp(touch("finished_assembly")),


rule qc:
    input:
        expand("{sample}/sequence_quality_control/finished_QC", sample=SAMPLES),
        read_counts="stats/read_counts.tsv",
        read_length_stats=(
            ["stats/insert_stats.tsv", "stats/read_length_stats.tsv"]
            if PAIRED_END
            else "stats/read_length_stats.tsv"
        ),
        report="reports/QC_report.html",
    output:
        temp(touch("finished_QC")),


# overwrite commands in rules/download.snakefile
onsuccess:
    print("ATLAS finished")
    print("The last rule shows you the main output files")


onerror:
    print("Note the path to the log file for debugging.")
    print("Documentation is available at: https://metagenome-atlas.readthedocs.io")
    print("Issues can be raised at: https://github.com/metagenome-atlas/atlas/issues")


### pepulate resources for rules that don't have

for r in workflow.rules:
    if not "mem" in r.resources:
        r.resources["mem"] = config["mem"]
    if not "time" in r.resources:
        if r.resources["mem"] == config["simplejob_mem"]:
            r.resources["time"] = config["runtime"]["simple_job"]
        else:
            r.resources["time"] = config["runtime"]["default"]


#
# # elif config.get("workflow") == "annotate":
# #     localrules: annotate
# #     rule annotate:
# #         input:
# #             expand("{sample}_annotations.txt", sample=SAMPLES),
# #             expand("{sample}/contig_stats.txt", sample=SAMPLES),
# #             #"reports/assembly_report.html" # not tested yet, but should work
# #
# #     include: "rules/annotate.smk"
#
# else:
#     raise Exception("Workflow %s is not a defined workflow." % config.get("workflow", "[no --workflow specified]"))
