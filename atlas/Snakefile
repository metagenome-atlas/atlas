import os
import re
import sys
import tempfile
from snakemake.utils import logger, min_version, update_config

from default_values import *

# get default values and update them with values specified in config file
default_config = make_default_config()
update_config(default_config, config)
config = default_config


# minimum required snakemake version
min_version("5.1.4")

def get_conda_envs_dir():
    if config.get("yaml_dir"):
        yaml_dir = config.get("yaml_dir")
    else:
        yaml_dir = os.path.join(os.path.dirname(os.path.abspath(workflow.snakefile)), "envs")
    if not os.path.exists(yaml_dir):
        sys.exit("Unable to locate the environmental dependencies file; tried %s" % yaml_dir)
    return yaml_dir


def get_temp_dir(config):
    if config.get("tmpdir"):
        tmp_dir = config["tmpdir"]
    else:
        tmp_dir = tempfile.gettempdir()
    return tmp_dir


def get_bin_summary_files(do_binning, samples):
    ret_str = ""
    if do_binning:
        ret_str = expand("{sample}/genomic_bins/{sample}.summary",
                      sample=samples)
    return ret_str


def get_shell_prefix(config, override={}):
    pfx = config.get("prefix")
    if not pfx:
        return ""

    keys = re.findall(r"__(\w+)__", pfx)
    for k in keys:
        if k in override:
            v = override[k]
        else:
            v = config.get(k, "")
        pfx = pfx.replace("__{key}__".format(key=k), str(v))
    return pfx


def update_config_file_paths(config):
    for sample in config["samples"]:
        try:
            # convert string into list
            if isinstance(config["samples"][sample]["fastq"], str):
                config["samples"][sample]["fastq"] = [config["samples"][sample]["fastq"]]
        # fastq is not required for annotation alone
        except KeyError:
            continue
    return config


def get_quality_controlled_reads(wildcards):
    """Gets quality controlled reads for two cases. When preprocessed with
    ATLAS, returns R1, R2 and se fastq files or just se. When preprocessed
    externaly and run ATLAS workflow assembly, we expect R1, R2 or se.
    """
    n_files = len(config["samples"][wildcards.sample]["fastq"])

    if config.get("workflow", "complete") == "annotate":
        # QA'd reads; the user wants to begin at assembly step
        if n_files == 2:
            fastq = dict(zip(['R1','R2'], config["samples"][wildcards.sample]["fastq"]))
        elif n_files == 1:
            if config["samples"][wildcards.sample].get("paired", False):
                logger.critical(("This protocol requires interleaved reads be "
                                 "reformatted into separate R1 and R2 files "
                                 "before execution. We recommend using "
                                 "reformat.sh from bbtools to convert."))
                sys.exit(1)
            fastq = {'se': config["samples"][wildcards.sample]["fastq"]}
    else:
        # reads that have gone through ATLAS QC
        fractions = ['R1', 'R2', 'se'] if n_files == 2 \
                        or config["samples"][wildcards.sample].get("paired", False) \
                        else ['se']
        fastq = dict(zip(fractions, expand("{sample}/sequence_quality_control/{sample}_QC_{fraction}.fastq.gz",
                                           fraction=fractions, sample=wildcards.sample)))
    return fastq


def input_params_for_bbwrap(wildcards, input):
    """This function generates the input flag needed for bbwrap for all cases
    possible for get_quality_controlled_reads.
    """
    if hasattr(input, 'R1') and hasattr(input, 'R2'):
        if hasattr(input, 'se'):
            flag = "in1={R1},{se} in2={R2},null".format(**input)
        else:
            flag = "in1={R1} in2={R2}".format(**input)
    elif hasattr(input, 'se'):
        flag = "in1={se}".format(**input)
    else:
        logger.critical(("File input expectation is one of: "
                         "1 file = single-end, "
                         "2 files = R1,R2, or"
                         "3 files = R1,R2,se"
                         "got: {n} files:\n{}").format('\n'.join(input),
                                                       n=len(input)))
        sys.exit(1)
    return flag


CONDAENV = get_conda_envs_dir()
EGGNOG_DIR  = os.path.join(os.path.dirname(os.path.realpath(config.get("diamond_db", "."))), "eggNOG")


if config.get("workflow") != "download":
    config = update_config_file_paths(config)
    TMPDIR = get_temp_dir(config)
    CHECKMDIR = os.path.join(os.path.dirname(os.path.realpath(config.get("diamond_db", "."))), "checkm")
    SAMPLES = [i for i in config["samples"].keys()]

    PAIRED_END = all([config["samples"][s].get("paired", False) \
                     or (len(config["samples"][s]["fastq"]) > 1) for s in SAMPLES])
    MULTIFILE_FRACTIONS = ['R1', 'R2', 'se'] if PAIRED_END else ['se']
    RAW_INPUT_FRACTIONS = ['R1', 'R2'] if PAIRED_END else ['se']
    REPORTS = ["reports/QC_report.html"]

# quality control of sequences only
if config.get("workflow", "complete") == "qc":
    # define paired end or not


    localrules: qc
    rule qc:
        input:
            expand("{sample}/sequence_quality_control/finished_QC", sample=SAMPLES),
            REPORTS

    include: "rules/qc.snakefile"

# quality control, assembly, annotation, and quantification
elif config.get("workflow", "complete") == "complete":
    # REPORTS = ["reports/QC_report.html", "reports/assembly_report.html"]
    REPORTS += ["reports/assembly_report.html"]

    if config.get("perform_genome_binning", True):
        # later update to include others as well as integrate concoct and metabat
        REPORTS.append("reports/bin_report_maxbin.html")
        # include this?
        # expand("{sample}/binning/{binner}/cluster_attribution.tsv",
        #        binner=config['binner'], sample =SAMPLES)
        include: "rules/binning.snakefile"
        ruleorder: align_reads_to_prefilter_contigs > bam_2_sam > align_reads_to_final_contigs

    localrules: all
    rule all:
        input:
            expand("{sample}/sequence_quality_control/finished_QC",
                sample=SAMPLES),
            # "finished_QC",
            expand("{sample}/assembly/contig_stats/prefilter_contig_stats.txt",
                sample=SAMPLES),
            expand("{sample}/assembly/contig_stats/final_contig_stats.txt",
                sample=SAMPLES),
            expand("{sample}/{sample}_annotations.txt", sample=SAMPLES),
            REPORTS

# checkpoint finished contigs, no annotation

    rule finish_assembly:
        input:
            expand("{sample}/sequence_quality_control/finished_QC",
                sample=SAMPLES),
            expand("{sample}/{sample}_contigs.fasta",
                sample=SAMPLES),
            expand("{sample}/sequence_alignment/{sample}.bam",
                sample=SAMPLES),
            expand("{sample}/assembly/contig_stats/postfilter_coverage_stats.txt", sample=SAMPLES),

    include: "rules/assemble.snakefile"
    include: "rules/qc.snakefile"
    include: "rules/gene_annotation.snakefile"


    if config.get("perform_genome_binning", True):


        rule merge_sample_tables:
            input:
                prokka = "{sample}/annotation/prokka/{sample}_plus.tsv",
                refseq = "{sample}/annotation/refseq/{sample}_tax_assignments.tsv",
                counts = "{sample}/annotation/feature_counts/{sample}_counts.txt",
                completeness = "{sample}/binning/checkm/maxbin/completeness.tsv",
                taxonomy = "{sample}/binning/checkm/maxbin/taxonomy.tsv"
            output:
                "{sample}/{sample}_annotations.txt"
            params:
                fastas = lambda wc: " --fasta ".join(glob("{sample}/binning/checkm/maxbin/{sample}.*.fasta".format(sample=wc.sample)))
            shell:
                "atlas merge-tables \
                     --counts {input.counts} \
                     --completeness {input.completeness} \
                     --taxonomy {input.taxonomy} \
                     --fasta {params.fastas} \
                     {input.prokka} \
                     {input.refseq} \
                     {output}"


    else:
        rule merge_sample_tables:
            input:
                prokka = "{sample}/annotation/prokka/{sample}_plus.tsv",
                refseq = "{sample}/annotation/refseq/{sample}_tax_assignments.tsv",
                counts = "{sample}/annotation/feature_counts/{sample}_counts.txt",
            output:
                "{sample}/{sample}_annotations.txt"
            shell:
                "atlas merge-tables \
                     --counts {input.counts} \
                     {input.prokka} \
                     {input.refseq} \
                     {output}"



elif config.get("workflow") == "binning":


    rule all:
        input:
            expand("{sample}/binning/{binner}/cluster_attribution.tsv",
                   binner=config['binner'], sample =SAMPLES),
            expand("{sample}/annotation/predicted_genes/{sample}.faa",
                   sample =SAMPLES),
            expand("reports/bin_report_{binner}.html", binner=config['binner'])


    include: "rules/binning.snakefile"
    include: "rules/assemble.snakefile"
    include: "rules/gene_annotation.snakefile"


elif config.get("workflow") == "download":

    DBDIR = os.path.realpath(config["db_dir"])
    CHECKMDIR = os.path.join(DBDIR, "checkm")
    CHECKM_ARCHIVE = "checkm_data_v1.0.9.tar.gz"
    # note: saving OG_fasta.tar.gz in order to not create secondary "success" file
    FILES = {"adapters.fa": "ae839dc79cfb855a1b750a0d593fe01e",
             "phiX174_virus.fa": "82516880142e8c89b466bc6118696c47",
             "refseq.db": "42b8976656f2cfd661b8a299d6e24c19",
             "refseq.dmnd": "c01facc7e397270ccb796ea799a09108",
             "refseq.tree": "469fcbeb15dd0d4bf8f1677682bde157",
             "silva_rfam_all_rRNAs.fa": "f102e35d9f48eabeb0efe9058559bc66",
             "OG_fasta.tar.gz": "8fc6ce2e055d1735dec654af98a641a4",
             "eggnog.db": "e743ba1dbc3ddc238fdcc8028968aacb",
             "eggnog_proteins.dmnd": "5efb0eb18ed4575a20d25773092b83b9",
             "og2level.tsv": "d35ffcc533c6e12be5ee8e5fd7503b84",
             CHECKM_ARCHIVE: "631012fa598c43fdeb88c619ad282c4d"}

    localrules: download
    rule download:
        input:
            expand("{dir}/{filename}", dir=DBDIR, filename=list(FILES.keys())),
            "%s/taxon_marker_sets.tsv" % CHECKMDIR,

    include: "rules/download.snakefile"

elif config.get("workflow") == "annotate":
    localrules: annotate
    rule annotate:
        input:
            expand("{sample}_annotations.txt", sample=SAMPLES),
            expand("{sample}/contig_stats.txt", sample=SAMPLES),
            #"reports/assembly_report.html" # not tested yet, but should work

    include: "rules/annotate.snakefile"

else:
    print("Workflow %s is not a defined workflow." % config.get("workflow", "[no --workflow specified]"),
          file=sys.stderr)
