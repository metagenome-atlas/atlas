import os
import re
import sys
import tempfile
import pandas as pd
import numpy as np
from pathlib import Path

from snakemake.utils import logger, min_version

workflow_folder = os.path.dirname(os.path.abspath(workflow.snakefile))

sys.path.append(os.path.join(workflow_folder, "scripts"))
import utils


# add default config
# comand line adds user config
configfile: os.path.join(workflow_folder, "config", "default_config.yaml")


# add defualt values from python (TODO: replace this)
from atlas.make_config import update_config as atlas_update_config

config = atlas_update_config(config)
from atlas.default_values import *  # LOAD HARDCODED values and EGGNOG_HEADER
from atlas.sample_table import load_sample_table

sampleTable = load_sample_table()

# minimum required snakemake version
min_version("6.1")


container: "docker://continuumio/miniconda3:4.4.10"


def get_temp_dir(config):
    if config.get("tmpdir"):
        tmp_dir = config["tmpdir"]
    else:
        tmp_dir = tempfile.gettempdir()
    return tmp_dir


def io_params_for_tadpole(io, key="in"):
    """This function generates the input flag needed for bbwrap/tadpole for all cases
    possible for get_quality_controlled_reads.

    params:
        io  input or output element from snakemake
        key 'in' or 'out'

        if io contains attributes:
            se -> in={se}
            R1,R2,se -> in1={R1},se in2={R2}
            R1,R2 -> in1={R1} in2={R2}

    """
    N = len(io)
    if N == 1:
        flag = f"{key}1={io[0]}"
    elif N == 2:
        flag = f"{key}1={io[0]} {key}2={io[1]}"
    elif N == 3:
        flag = f"{key}1={io[0]},{io[2]} {key}2={io[1]}"
    else:
        logger.error(
            (
                "File input/output expectation is one of: "
                "1 file = single-end/ interleaved paired-end "
                "2 files = R1,R2, or"
                "3 files = R1,R2,se"
                "got: {n} files:\n{}"
            ).format("\n".join(io), n=len(io))
        )
        sys.exit(1)
    return flag


def input_params_for_bbwrap(input):
    if len(input) == 3:
        return f"in1={input[0]},{input[2]} in2={input[1]},null"
    else:
        return io_params_for_tadpole(input)


# if config.get("workflow") != "download":

#    config = update_config_file_paths(config)
TMPDIR = get_temp_dir(config)
SAMPLES = sampleTable.index.values
SKIP_QC = False


# GROUPS = sampleTable.BinGroup.unique()
def get_alls_samples_of_group(wildcards):
    group_of_sample = sampleTable.loc[wildcards.sample, "BinGroup"]

    return list(sampleTable.loc[sampleTable.BinGroup == group_of_sample].index)


PAIRED_END = sampleTable.columns.str.contains("R2").any() or config.get(
    "interleaved_fastqs", False
)


colum_headers_QC = sampleTable.columns[sampleTable.columns.str.startswith("Reads_QC_")]
if len(colum_headers_QC) >= 1:
    MULTIFILE_FRACTIONS = list(colum_headers_QC.str.replace("Reads_QC_", ""))

    if (len(MULTIFILE_FRACTIONS) == 1) and config.get("interleaved_fastqs", False):
        MULTIFILE_FRACTIONS = ["R1", "R2"]

else:
    MULTIFILE_FRACTIONS = ["R1", "R2", "se"] if PAIRED_END else ["se"]

colum_headers_raw = sampleTable.columns[
    sampleTable.columns.str.startswith("Reads_raw_")
]
if len(colum_headers_raw) == 0:
    SKIP_QC = True

    logger.info("Didn't find raw reads in sampleTable - skip QC")
    RAW_INPUT_FRACTIONS = MULTIFILE_FRACTIONS
else:
    RAW_INPUT_FRACTIONS = ["R1", "R2"] if PAIRED_END else ["se"]


if (len(colum_headers_raw) == 0) and (len(colum_headers_QC) == 0):
    raise IOError(
        "Either raw reas or QC reads need to be in the sample table. "
        "I din't find any columnns with 'Reads_raw_<fraction>' or 'Reads_QC_<fraction>'  "
    )


class FileNotInSampleTableException(Exception):
    """
    Exception with sampleTable
    """

    def __init__(self, message):
        super(FileNotInSampleTableException, self).__init__(message)


def get_files_from_sampleTable(sample, Headers):
    """
    Function that gets some filenames form the sampleTable for a given sample and Headers.
    It checks various possibilities for errors and throws either a
    FileNotInSampleTableException or a IOError, when something went really wrong.
    """

    if not (sample in sampleTable.index):
        raise IOError(f"Sample name {sample} is not in sampleTable")

    Error_details = f"\nsample: {sample}\nFiles: {Headers}"

    if type(Headers) == str:
        Headers = [Headers]

    NheadersFound = sampleTable.columns.isin(Headers).sum()

    if NheadersFound == 0:
        raise FileNotInSampleTableException(
            f"None of the Files ar in sampleTable, they should be added to the sampleTable later in the workflow"
            + Error_details
        )
    elif NheadersFound < len(Headers):
        raise IOError(
            f"Not all of the Headers are in sampleTable, found only {NheadersFound}, something went wrong."
            + Error_details
        )

    files = sampleTable.loc[sample, Headers]

    if files.isnull().all():
        raise FileNotInSampleTableException(
            "The following files were not available for this sample in the SampleTable"
            + Error_details
        )

    elif files.isnull().any():
        raise IOError(
            f"Not all of the files are in sampleTable, something went wrong."
            + Error_details
        )

    return list(files)


def get_quality_controlled_reads(wildcards, include_se=False):
    """
    Gets quality controlled reads.
    R1 and R1 or se are returned as a dict.

    if the files are not in the sample tible impute default path produced with atlas.
    set

    """

    Fractions = MULTIFILE_FRACTIONS

    if config.get("interleaved_fastqs", False) and SKIP_QC:
        Fractions = ["se"]

    elif not include_se:
        # get only R1 and R2 or se
        Fractions = Fractions[: min(len(Fractions), 2)]

    try:
        QC_Headers = ["Reads_QC_" + f for f in Fractions]
        return get_files_from_sampleTable(wildcards.sample, QC_Headers)

    except FileNotInSampleTableException:
        # return files as named by atlas pipeline
        return expand(
            "{sample}/sequence_quality_control/{sample}_QC_{fraction}.fastq.gz",
            fraction=Fractions,
            sample=wildcards.sample,
        )


wildcard_constraints:
    binner="[A-Za-z]+",


include: "rules/download.smk"  # contains hard coded variables
include: "rules/qc.smk"
include: "rules/assemble.smk"
include: "rules/binning.smk"
include: "rules/genomes.smk"
include: "rules/dram.smk"
include: "rules/genecatalog.smk"
include: "rules/sra.smk"
include: "rules/gtdbtk.smk"
include: "rules/cobinning.smk"
include: "rules/strains.smk"


CONDAENV = "envs"  # overwrite definition in download.smk


localrules:
    all,
    qc,
    assembly_one_sample,
    assembly,
    genomes,


rule all:
    input:
        "finished_QC",
        "finished_assembly",
        "finished_binning",
        "finished_genomes",
        "finished_genecatalog",
        "genomes/annotations/gene2genome.parquet",


def get_gene_catalog_input():
    annotation_file_names = {
        "eggNOG": "Genecatalog/annotations/eggNOG.parquet",
        "dram": "Genecatalog/annotations/dram",
        "single_copy": expand(
            "Genecatalog/annotation/single_copy_genes_{domain}.tsv",
            domain=["bacteria", "archaea"],
        ),
    }

    annotations_requested = config.get("gene_annotations", [])

    try:
        annotations_files = ["Genecatalog/counts/median_coverage.h5"] + [
            annotation_file_names[key] for key in annotations_requested
        ]

    except Exception as e:
        raise IOError(
            "Error in gene_annotations requested, check config file 'gene_annotations' "
        ) from e

    return annotations_files


rule genecatalog:
    input:
        "Genecatalog/gene_catalog.fna",
        "Genecatalog/gene_catalog.faa",
        "Genecatalog/clustering/orf_info.parquet",
        get_gene_catalog_input(),
    output:
        touch("finished_genecatalog"),


rule strains:
    input:
        "strains/comparison",


def get_genome_annotations():
    annotation_file_names = {
        "gtdb_tree": "genomes/tree/finished_gtdb_trees",
        "gtdb_taxonomy": "genomes/taxonomy/gtdb_taxonomy.tsv",
        "genes": "genomes/annotations/genes/predicted",
        "kegg_modules": "genomes/annotations/dram/kegg_modules.tsv",
        "dram": "genomes/annotations/dram/distil",
    }

    annotations_requested = config["annotations"]

    try:
        annotations_files = ["genomes/genome_quality.tsv"] + [
            annotation_file_names[an] for an in annotations_requested
        ]

    except Exception as e:
        raise IOError(
            "Error in annotations requested, check config file 'annotations' "
        ) from e

    return annotations_files


rule genomes:
    input:
        "genomes/counts/median_coverage_genomes.parquet",
        "genomes/counts/counts_genomes.parquet",
        "genomes/clustering/allbins2genome.tsv",
        "reports/genome_mapping/results.html",
        *get_genome_annotations(),
        "finished_binning",
    output:
        touch("finished_genomes"),


rule quantify_genomes:
    input:
        "genomes/counts/median_coverage_genomes.parquet",
        expand("genomes/counts/counts_genomes.parquet"),


rule binning:
    input:
        expand(
            "{sample}/binning/{binner}/cluster_attribution.tsv",
            binner=config["final_binner"],
            sample=SAMPLES,
        ),
        expand("Binning/{binner}/report.html", binner=config["final_binner"]),
        "finished_assembly",
    output:
        touch("finished_binning"),


rule assembly_one_sample:
    input:
        "{sample}/{sample}_contigs.fasta",
        "{sample}/sequence_alignment/{sample}.bam",
        "{sample}/assembly/contig_stats/postfilter_coverage_stats.txt",
        "{sample}/assembly/contig_stats/prefilter_contig_stats.txt",
        "{sample}/assembly/contig_stats/final_contig_stats.txt",
    output:
        touch("{sample}/finished_assembly"),


rule assembly:
    input:
        expand("{sample}/finished_assembly", sample=SAMPLES),
        "reports/assembly_report.html",
        "finished_QC",
    output:
        touch("finished_assembly"),


rule qc:
    input:
        expand("{sample}/sequence_quality_control/finished_QC", sample=SAMPLES),
        read_counts="stats/read_counts.tsv",
        read_length_stats=(
            ["stats/insert_stats.tsv", "stats/read_length_stats.tsv"]
            if PAIRED_END
            else "stats/read_length_stats.tsv"
        ),
        report="reports/QC_report.html",
    output:
        touch("finished_QC"),


# overwrite commands in rules/download.snakefile
onsuccess:
    print("ATLAS finished")
    print("The last rule shows you the main output files")


onerror:
    print("Note the path to the log file for debugging.")
    print("Documentation is available at: https://metagenome-atlas.readthedocs.io")
    print("Issues can be raised at: https://github.com/metagenome-atlas/atlas/issues")


### pepulate resources for rules that don't have

for r in workflow.rules:
    if not "mem" in r.resources:
        r.resources["mem"] = config["mem"]

    # add time if ot present. Simple jobs use simple time

    if "time_min" not in r.resources:
        if "time" in r.resources:
            r.resources["time_min"] = r.resources["time"] * 60
        else:
            if r.resources["mem"] <= config["simplejob_mem"]:
                r.resources["time_min"] = config["runtime"]["simplejob"] * 60
            else:
                r.resources["time_min"] = config["runtime"]["default"] * 60

    # convert to new units
    r.resources["mem_mb"] = r.resources["mem"] * 1000
    if not "runtime" in r.resources:
        r.resources["runtime"] = r.resources["time_min"] * 60
